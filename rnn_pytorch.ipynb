{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using torchtext datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "train_ds = IMDB('./data/imdb/train', split='train')\n",
    "train_ds = list(train_ds)\n",
    "\n",
    "test_ds = IMDB('./data/imdb/test', split='test')\n",
    "test_dataset = list(test_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "train_dataset, valid_dataset = random_split(train_ds, [20000, 5000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab_size: 69646\n"
     ]
    }
   ],
   "source": [
    "from text_tokenizer import build_vocab\n",
    "token_counts = build_vocab(train_dataset)\n",
    "vocab_size = len(token_counts)\n",
    "print('vocab_size:', vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the encoding dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import vocab\n",
    "from collections import OrderedDict\n",
    "sorted_tokens = sorted(token_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "tokens_dict = OrderedDict(sorted_tokens)\n",
    "text_vocab = vocab(tokens_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vocab.insert_token('<pad>', 0)\n",
    "text_vocab.insert_token('<unk>', 1)\n",
    "text_vocab.set_default_index(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text_tokenizer import tokenizer\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "def text_pipeline(text):\n",
    "    return [text_vocab[token] for token in tokenizer(text)]\n",
    "\n",
    "def label_pipeline(label):\n",
    "    return 1. if label == 'pos' else 0.\n",
    "\n",
    "def collate_batch(batch):\n",
    "    labels, texts, texts_lenghts = [], [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(label_pipeline(label))\n",
    "        procesed_text = text_pipeline(text)\n",
    "        texts.append(torch.tensor(procesed_text, dtype=torch.int32))\n",
    "        texts_lenghts.append(len(procesed_text))\n",
    "    labels = torch.tensor(labels)\n",
    "    texts_lenghts = torch.tensor(texts_lenghts)\n",
    "    texts = pad_sequence(texts, batch_first=True)    \n",
    "    return texts, labels, texts_lenghts   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=20)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=20)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 62, 8])\n",
      "torch.Size([1, 32, 8])\n",
      "<class 'torch.Tensor'>\n",
      "<class 'tuple'>\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "features_size = 30\n",
    "batch_size = 32\n",
    "sequences_len = 62\n",
    "rnn = nn.RNN(input_size=features_size, hidden_size=8, num_layers=1, batch_first=True)\n",
    "sample_batch = torch.rand(batch_size, sequences_len, features_size)\n",
    "a, b = rnn(sample_batch)\n",
    "\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "\n",
    "lstm = nn.LSTM(input_size=features_size, hidden_size=8, num_layers=1, batch_first=True)\n",
    "c, d = lstm(sample_batch)\n",
    "print(type(c))\n",
    "print(type(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   96,    58,  6260,  ...,     0,     0,     0],\n",
       "         [ 9790, 12012,    59,  ...,     0,     0,     0],\n",
       "         [   10,    14,     9,  ...,     0,     0,     0],\n",
       "         ...,\n",
       "         [   10,   509,    11,  ...,     0,     0,     0],\n",
       "         [   10,  2002,    66,  ...,     0,     0,     0],\n",
       "         [    2,    63,   137,  ...,     0,     0,     0]], dtype=torch.int32),\n",
       " tensor([0., 0., 1., 1., 0., 0., 1., 1., 1., 0., 0., 0., 0., 0., 0., 1., 1., 0.,\n",
       "         1., 0., 0., 1., 0., 0., 0., 1., 1., 1., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "         1., 0., 0., 0., 1., 1., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0.,\n",
       "         1., 0., 1., 1., 0., 1., 0., 0., 0., 0.]),\n",
       " tensor([269, 243,  83,  76, 140, 127, 251, 361,  66,  93, 117, 214, 231, 117,\n",
       "         691, 171, 177, 108, 271, 119, 134,  49, 126, 154, 316, 142, 177, 195,\n",
       "         448, 157, 142, 226, 624, 125, 172,  99, 349, 272, 337, 456, 380, 189,\n",
       "         106, 486, 133, 216, 208, 492, 276, 126, 143, 118, 293, 140, 201, 134,\n",
       "         359, 143, 905, 135, 138, 132, 471, 112]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_size = 16\n",
    "vocab_size = len(token_counts)\n",
    "emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=features_size, padding_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentimentModel0(\n",
      "  (emb): Embedding(69648, 20, padding_idx=0)\n",
      "  (rnn): LSTM(20, 64, batch_first=True)\n",
      "  (fc): Linear(in_features=64, out_features=64, bias=True)\n",
      "  (fc_relu): ReLU()\n",
      "  (fc_out): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class SentimentModel0(nn.Module):\n",
    "    def __init__(self, vocab_size, features_size=20, hidden_size=64, fc_size=64):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=features_size, padding_idx=0)\n",
    "        self.rnn = nn.LSTM(input_size=features_size, hidden_size=hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, fc_size)\n",
    "        self.fc_relu = nn.ReLU()\n",
    "        self.fc_out = nn.Linear(fc_size, 1)\n",
    "\n",
    "    def forward(self, texts, texts_lengths):\n",
    "        out = self.emb(texts)\n",
    "        out = nn.utils.rnn.pack_padded_sequence(out, texts_lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hidden, cell) = self.rnn(out)\n",
    "        out = hidden[-1, :, :]\n",
    "        \n",
    "        out = self.fc(out)\n",
    "        out = self.fc_relu(out)\n",
    "        out = self.fc_out(out)\n",
    "        return out    \n",
    "\n",
    "model = SentimentModel0(vocab_size+2)\n",
    "print(model)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.functional import F\n",
    "from torch.optim.lr_scheduler import MultiStepLR\n",
    "\n",
    "\n",
    "def train(model, dataloader, optimizer, device, loss_fn, progress_bar):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    for text_batch, labels_batch, lengths_batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        text_batch = text_batch.to(device)\n",
    "        labels_batch = labels_batch.to(device)\n",
    "        lengths_batch = lengths_batch.to(device)\n",
    "        y_pred = model(text_batch, lengths_batch)[:, 0]\n",
    "        loss = loss_fn(y_pred, labels_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        progress_bar.update(1)\n",
    "        epoch_loss += loss.item() * text_batch.size(0)\n",
    "        epoch_acc += (torch.sigmoid(y_pred).round() == labels_batch).float().sum().item()\n",
    "    return epoch_acc/num_samples, epoch_loss/num_samples    \n",
    "        \n",
    "        \n",
    "def evaluate(model, dataloader, device, loss_fn):\n",
    "    model.eval()\n",
    "    epoch_loss, epoch_acc = 0., 0.\n",
    "    num_samples = len(dataloader.dataset)\n",
    "    with torch.no_grad():\n",
    "        for text_batch, labels_batch, lengths_batch in dataloader:\n",
    "            text_batch = text_batch.to(device)\n",
    "            labels_batch = labels_batch.to(device)\n",
    "            lengths_batch = lengths_batch.to(device)\n",
    "            y_pred = model(text_batch, lengths_batch)[:, 0]\n",
    "            loss = loss_fn(y_pred, labels_batch)\n",
    "            epoch_loss += loss.item() * text_batch.size(0)\n",
    "            epoch_acc += (torch.sigmoid(y_pred).round() == labels_batch).float().sum().item()\n",
    "    return epoch_acc/num_samples, epoch_loss/num_samples   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0debf7b697c444bb8c38f5cfae5b2cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9390 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Train Loss: 0.1619, Train Acc: 0.9416, Valid Loss: 0.4344, Valid Acc: 0.8570\n",
      "Epoch: 1, Train Loss: 0.1462, Train Acc: 0.9477, Valid Loss: 0.4961, Valid Acc: 0.8488\n",
      "Epoch: 2, Train Loss: 0.1244, Train Acc: 0.9574, Valid Loss: 0.4336, Valid Acc: 0.8482\n",
      "Epoch: 3, Train Loss: 0.1061, Train Acc: 0.9640, Valid Loss: 0.4695, Valid Acc: 0.8530\n",
      "Epoch: 4, Train Loss: 0.1122, Train Acc: 0.9613, Valid Loss: 0.4966, Valid Acc: 0.8408\n",
      "Epoch: 5, Train Loss: 0.0755, Train Acc: 0.9759, Valid Loss: 0.4795, Valid Acc: 0.8474\n",
      "Epoch: 6, Train Loss: 0.0578, Train Acc: 0.9830, Valid Loss: 0.5709, Valid Acc: 0.8562\n",
      "Epoch: 7, Train Loss: 0.0521, Train Acc: 0.9836, Valid Loss: 0.6306, Valid Acc: 0.8568\n",
      "Epoch: 8, Train Loss: 0.0657, Train Acc: 0.9781, Valid Loss: 0.6715, Valid Acc: 0.8480\n",
      "Epoch: 9, Train Loss: 0.0352, Train Acc: 0.9900, Valid Loss: 0.7135, Valid Acc: 0.8476\n",
      "Epoch: 10, Train Loss: 0.0298, Train Acc: 0.9914, Valid Loss: 0.6995, Valid Acc: 0.8564\n",
      "Epoch: 11, Train Loss: 0.0263, Train Acc: 0.9928, Valid Loss: 0.7645, Valid Acc: 0.8522\n",
      "Epoch: 12, Train Loss: 0.0169, Train Acc: 0.9960, Valid Loss: 0.9035, Valid Acc: 0.8452\n",
      "Epoch: 13, Train Loss: 0.0155, Train Acc: 0.9953, Valid Loss: 0.8704, Valid Acc: 0.8490\n",
      "Epoch: 14, Train Loss: 0.0210, Train Acc: 0.9938, Valid Loss: 0.9004, Valid Acc: 0.8520\n",
      "Epoch: 15, Train Loss: 0.0108, Train Acc: 0.9971, Valid Loss: 0.9371, Valid Acc: 0.8508\n",
      "Epoch: 16, Train Loss: 0.0349, Train Acc: 0.9885, Valid Loss: 0.8432, Valid Acc: 0.8500\n",
      "Epoch: 17, Train Loss: 0.0072, Train Acc: 0.9981, Valid Loss: 0.9735, Valid Acc: 0.8546\n",
      "Epoch: 18, Train Loss: 0.0037, Train Acc: 0.9991, Valid Loss: 1.0174, Valid Acc: 0.8538\n",
      "Epoch: 19, Train Loss: 0.0258, Train Acc: 0.9920, Valid Loss: 0.9709, Valid Acc: 0.8540\n",
      "Epoch: 20, Train Loss: 0.0100, Train Acc: 0.9973, Valid Loss: 0.9785, Valid Acc: 0.8534\n",
      "Epoch: 21, Train Loss: 0.0037, Train Acc: 0.9991, Valid Loss: 1.1298, Valid Acc: 0.8368\n",
      "Epoch: 22, Train Loss: 0.0205, Train Acc: 0.9937, Valid Loss: 0.9720, Valid Acc: 0.8490\n",
      "Epoch: 23, Train Loss: 0.0047, Train Acc: 0.9989, Valid Loss: 1.0398, Valid Acc: 0.8510\n",
      "Epoch: 24, Train Loss: 0.0096, Train Acc: 0.9972, Valid Loss: 1.0295, Valid Acc: 0.8524\n",
      "Epoch: 25, Train Loss: 0.0022, Train Acc: 0.9995, Valid Loss: 1.1761, Valid Acc: 0.8510\n",
      "Epoch: 26, Train Loss: 0.0059, Train Acc: 0.9986, Valid Loss: 1.1821, Valid Acc: 0.8556\n",
      "Epoch: 27, Train Loss: 0.0004, Train Acc: 1.0000, Valid Loss: 1.2411, Valid Acc: 0.8556\n",
      "Epoch: 28, Train Loss: 0.0002, Train Acc: 1.0000, Valid Loss: 1.3250, Valid Acc: 0.8556\n",
      "Epoch: 29, Train Loss: 0.0001, Train Acc: 1.0000, Valid Loss: 1.3747, Valid Acc: 0.8560\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "device = torch.device('cuda')\n",
    "model.to(device)\n",
    "epochs = 30\n",
    "progress_bar = tqdm(range(epochs*len(train_dataloader)))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_acc, train_loss = train(model, train_dataloader, optimizer, device, loss_fn, progress_bar)\n",
    "    valid_acc, valid_loss = evaluate(model, valid_dataloader, device, loss_fn)\n",
    "    print(f'Epoch: {epoch}, Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Valid Loss: {valid_loss:.4f}, Valid Acc: {valid_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6cdbf094c3e2d11b30478a1f6f10290d4ee78e6b46f57992373b88ecd109df83"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
